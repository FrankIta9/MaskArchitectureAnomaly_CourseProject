# ============================================================================
# FROZEN BACKBONE + DECODER FINE-TUNING per Outlier Exposure
# ============================================================================
#
# STRATEGIA:
# - Backbone (ViT) CONGELATO → preserva feature semantiche Cityscapes
# - Solo decoder trainable (~10M params vs 95M) → fine-tuning leggero
# - LR più alto (5e-5) possibile perché solo decoder cambia
# - Training 10x più veloce, più stabile, zero rischio overfitting
#
# VANTAGGI:
# 1. Mantiene 100% mIoU Cityscapes (backbone intatto)
# 2. Decoder impara solo a riconoscere outliers COCO come "no object"
# 3. Convergenza rapida (poche epochs sufficienti)
# 4. Allineato con best practices OE (RbA paper)
#
# PARAMETRI TRAINABILI:
# - Scale blocks (cross-attention tra ViT e queries)
# - Learned queries (embeddings semantici)
# - Class head (classifica in-distribution vs "no object")
# - Mask head (genera maschere per ogni query)
#
# ============================================================================

trainer:
  max_epochs: 30  # Ridotto: convergenza più rapida con solo decoder
  
  # Batch size più grande possibile con decoder-only training
  accumulate_grad_batches: 1  # Effective = batch_size
  precision: "16-mixed"
  
  # Checkpoint su Google Drive
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: "/content/drive/MyDrive/eomt_oe_frozen_backbone"
        filename: "eomt_frozen-{epoch:03d}-{metrics/val_iou_all:.4f}"
        monitor: "metrics/val_iou_all"
        mode: "max"
        save_top_k: 3
        save_last: true
        every_n_epochs: 1
  
  logger:
    class_path: lightning.pytorch.loggers.wandb.WandbLogger
    init_args:
      resume: allow
      project: "eomt"
      name: "OE_FrozenBackbone_lr1e6_LogitNorm_weightedScale"

model:
  class_path: training.mask_classification_semantic.MaskClassificationSemantic
  init_args:
    # Pesi Cityscapes puliti (riparti da capo con distribuzione pesata)
    ckpt_path: "/content/drive/MyDrive/eomt_cityscapes.bin"
    # IMPORTANTE: Riparti da pesi Cityscapes puliti (non da epoch 16) per evitare disimparamento
    # Con distribuzione pesata + parametri conservativi, il training sarà più stabile
    
    attn_mask_annealing_enabled: True
    attn_mask_annealing_start_steps: [3317, 8292, 13268]
    attn_mask_annealing_end_steps: [6634, 11609, 16585]
    
    # ===== LR ULTRA-CONSERVATIVO =====
    # 1e-6 = estremamente conservativo anche per decoder-only
    # Garantisce massima stabilità + compatibilità con logit_norm
    lr: 1e-6
    llrd: 0.8  # Ignorato (backbone frozen), ma lasciato per compatibility
    
    # ===== ENERGY OOD CON WARMUP =====
    # IMPORTANTE: Se riparti da pesi già addestrati (es. epoch 16), imposta warmup_start_epoch
    # Questo fa "saltare" il warmup perché il modello è già pronto
    energy_ood_enabled: true
    energy_ood_max_weight: 0.002  # Conservative (come ultraconservative)
    energy_warmup_epochs: 10  # Warmup medio
    energy_warmup_start_epoch: 0  # REVERT a 0 (riparti da capo con energy warmup normale)
    # IMPORTANTE: Se hai estratto pesi da epoch 16, riparti da capo con energy warmup graduale
    # Questo evita instabilità da energy loss attiva subito su modello non esposto a energy loss
    # All'epoch 0 → adjusted=0 (<10) → Energy DISABLED fino a epoch 10 (come previsto)
    max_epochs: 30
    
    # ===== LOGIT NORMALIZATION ENABLED =====
    # Normalizza logits per migliorare calibrazione anomaly scores
    # Funzionava bene nei test precedenti
    logit_norm_enabled: true
    logit_norm_tau: 0.04
    logit_norm_eps: 1e-6
    
    network:
      class_path: models.eomt.EoMT
      init_args:
        num_q: 100
        num_blocks: 3
        encoder:
          class_path: models.vit.ViT
          init_args:
            backbone_name: vit_base_patch14_reg4_dinov2

data:
  class_path: datasets.cityscapes_semantic_with_oe.CityscapesSemanticWithOE
  init_args:
    path: "/content/drive/MyDrive/Cityscapes"
    
    # Batch size ridotto per limiti memoria VRAM
    batch_size: 2  # Ridotto da 6 per limiti memoria
    num_workers: 8
    
    img_size: [1024, 1024]
    num_classes: 19
    color_jitter_enabled: true
    scale_range: [0.5, 2.0]
    check_empty_targets: true
    
    # ===== OUTLIER EXPOSURE OTTIMIZZATO =====
    coco_path: "/content/drive/MyDrive/coco2017_zips"
    coco_split: "train2017"  # Usa train2017 per più varietà (gestiamo noi la distribuzione pesata)
    # MOTIVAZIONE: Con distribuzione pesata multi-scale (35% piccoli, 35% medi, 30% grandi)
    # gestiamo noi la varietà, quindi train2017 (~500k oggetti) è perfetto per più varietà
    # Vantaggi: ~500k oggetti validi vs ~18k (val2017) → molto più varietà senza problemi di stabilità
    # Con 1 oggetto/batch al 15%, uso ~445 oggetti/epoch → train2017 evita ripetizioni e migliora learning
    use_coco_zip: true
    
    # ===== DISTRIBUZIONE PESATA MULTI-SCALE BILANCIATA =====
    # PROBLEMA IDENTIFICATO: RoadObstacle21 stava disimparando PRIMA della distribuzione pesata
    # - RoadObstacle21: anomalie GRANDI/MEDIE (10-40%) → FPR@95TPR passato da 0.51% a 10.98% (20x peggioramento!)
    # - fs_static/LostFound: anomalie PICCOLISSIME (1-5%) → basso AUPRC
    # - RoadAnomaly: anomalie MISTE (5-40%) → migliorato (+1.84%)
    #
    # CAUSE PROBABILI DEL DISIMPARAMENTO:
    # 1. paste_probability: 0.20 (era 0.15) → TROPPO ALTO, confonde il modello
    # 2. min_scale/max_scale: 0.03/0.08 (erano 0.08/0.15) → TROPPO PICCOLI per RoadObstacle21
    # 3. coco_min_area: 600 (era 800) → TROPPO BASSO, oggetti di bassa qualità
    # 4. train2017 SENZA distribuzione pesata → troppa varietà casuale senza controllo
    #
    # SOLUZIONE: Parametri CONSERVATIVI + Distribuzione pesata BILANCIATA + train2017
    # - Ridurre paste_probability a 0.15 (valore originale che funzionava)
    # - Distribuzione pesata (35% piccoli, 35% medi, 30% grandi) CONTROLLA la varietà train2017
    # - train2017 con distribuzione pesata → più varietà MA controllata (best of both worlds)
    # - Aumentare coco_min_area a 800 per qualità migliore
    paste_probability: 0.15  # RIDOTTO a 0.15 (valore originale che funzionava bene con RoadObstacle21)
    min_objects: 1
    max_objects: 1           # Solo 1 oggetto per volta (più stabile, evita ComplexFloat errors)
    
    # Distribuzione pesata multi-scale BILANCIATA (35% piccoli, 35% medi, 30% grandi)
    # Priorità a preservare RoadObstacle21 (30% grandi) mentre miglioriamo fs_static/LostFound (35% piccoli)
    use_weighted_scale: true  # Abilita distribuzione pesata bilanciata
    scale_ranges:
      - [0.02, 0.05]   # Piccoli: 2-5% immagine (35% prob) → matcha fs_static/LostFound
      - [0.05, 0.10]   # Medi: 5-10% immagine (35% prob) → matcha RoadAnomaly piccole/medie
      - [0.10, 0.18]   # Grandi: 10-18% immagine (30% prob) → PRESERVA RoadObstacle21 (anomalie grandi!)
    scale_weights: [0.35, 0.35, 0.30]  # 35% piccoli, 35% medi, 30% grandi (BILANCIATO verso grandi)
    
    # Parametri legacy (ignorati se use_weighted_scale=true):
    min_scale: 0.03          # Ignorato se use_weighted_scale=true
    max_scale: 0.08          # Ignorato se use_weighted_scale=true
    coco_min_area: 800       # AUMENTATO a 800 (valore originale) per qualità migliore e stabilità
